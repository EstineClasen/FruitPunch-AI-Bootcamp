{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EstineClasen/FruitPunch-AI-Bootcamp/blob/main/1_3_Convolutional_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zpOMNIqo_OkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Networks\n",
        "This challenge is going to be a bit more, well... challenging. You are going to process image data and use Convolutional Neural Networks. These types of algorithms are designed to use multidimensional data where the spatial relations between the dimensions are relevant. Or in other words, good with pretty pictures.\n",
        "\n",
        "Today we are going to use the MNIST dataset, probably the most famous dataset in all of AI. It consists of 28x28 pixel pictures of handwritten numbers in greyscale (black & white).\n",
        "\n",
        "We are going to use some real Deep Learning here that benefits from GPU acceleration. Luckily Colab can provide us with a free GPU to train the neural networks faster. You can activate GPU acceleration in the Runtime tab: Runtime -> Change runtime type -> Select GPU from the dropdown\n",
        "\n"
      ],
      "metadata": {
        "id": "wFpJ9cbY_MgB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_F82xoK_INS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ea2044-94ce-4608-bd62-069b233efc9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "#(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View example digit\n",
        "index = np.random.randint(0,60000)\n",
        "plt.imshow(X_train[index])\n",
        "print(y_train[index])"
      ],
      "metadata": {
        "id": "ELbl-BK__Yhk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "daba4d49-6d6c-44e4-8da8-fcd1ba4447ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping the data\n",
        "CNNs expect the data to be in a specific shape depending on the number of dimensions. Since we work with image data, we will use 2D CNN layers. The expected shape that would be as follows (number_of_samples, dimension_size_1, dimension_size_2, channel). Looking at X_train. The shape above the first 3 is easy to derive. The number of training samples is 60000 and the images are 28*28 and therefore those would be the two dimension sizes. Channel would basically be the number of spectral wavelengths are in the data. For colored images this is 3 (red, green, blue) for greyscale images this would just be 1, and for multispectral satellite images, this could be many more. In this case, we have black and white images, so we end up with the desired X_train shape of (60000,28,28,1). This is not the shape of our data as can be seen above, that would be (60000,28,28). This is because it has the intensity value in the second dimension instead of it being just a coordinate and an extra dimension for the intensity. Luckily we can easily reshape the data to our desired format."
      ],
      "metadata": {
        "id": "LZvsMNxd_ewY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put it into suitable shape\n",
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)"
      ],
      "metadata": {
        "id": "uDjrnzVa_fUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use one hot encoding to covert the labels\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_train)\n",
        "y_train = lb.transform(y_train)\n",
        "y_test = lb.transform(y_test)\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "xM51x7pm_kFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3455c3a3-6295-4407-9c46-d067f99c56cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 1 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture\n",
        "Now for the architecture\n",
        "\n",
        "- This time we defined the input in a separate Input Layer, which results in exactly the same as when you define input_shape in the first layer.\n",
        "- Then we do a single Convolutional layer\n",
        "- The last layer should stay a Dense layer with the number of classes for neurons just like with the previous. However, we can not just push 2D data into a 1D Dense layer therefore we need to Flatten the data using a Flatten layer.\n",
        "\n",
        "\n",
        "Configuring the Conv2D layers will require some fiddling, here is the documentation:\n",
        "https://keras.io/api/layers/convolution_layers/convolution2d/\n",
        "<br><br>\n",
        "You should at least define a **filter size**. To keep it simple, this number determines the amount of information that will be collected per output \"pixel\" in the output. So a higher number means more space for information but also longer compute times and larger model size. \n",
        "<br><br>\n",
        "Then you should also define a **kernel size** (also known as window) Which basically determines how much information is condensed into an output \"pixel\". This means that larger kernels result in a larger segment of the image will be cramped into a smaller resulting image. So here larger means less information pushed to the next layer. \n",
        "\n",
        "**Warning:** When adding Conv layers and changing kernel sizes you impact the shape of de \"image\" that is passed to the next layers, often making it smaller. This means that when you stack too many layers and/or use large kernel sizes, the image at some point will become smaller than 0x0 resulting in a crash. You can simply calculate the effect each propagating layer will have on the image but you can also pay extra attention to the model summary since that will show the resulting image dimensions after each layer.\n",
        " \n"
      ],
      "metadata": {
        "id": "siK3d9Wr_nA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, \n",
        "                                     Dense, Dropout, BatchNormalization, Activation)"
      ],
      "metadata": {
        "id": "JmbIVsFd_kj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_cnn(input_shape, num_classes):\n",
        "    # Build the architecture\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # first CONV => RELU => CONV => RELU \n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\",\n",
        "        input_shape=input_shape))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # second CONV => RELU => CONV => RELU\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # first (and only) set of FC => RELU layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # softmax classifier\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "W1dUl9WS_pxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the network\n",
        "For the optimizer we will use classic Stochastic Gradient Descent. There exist more advanced optimizers like Adam, but we will skip those for now. The loss defines how we punish the network for a mistake. Without going into the details Categorical Crossentropy is generally used with classification."
      ],
      "metadata": {
        "id": "KvIwrB6H_09e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "input_shape = X_train.shape[1:]\n",
        "num_classes = y_train.shape[-1]\n",
        "\n",
        "model = define_cnn(input_shape, num_classes)\n",
        "#model.summary()\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train and test\n",
        "H = model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "mIA9lYi6_1dg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ddbda9-de2e-4dec-c623-87e3e44e1a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 64)        18496     \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 28, 28, 64)        0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 28, 28, 64)        36928     \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 28, 28, 64)        0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 50176)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               25690624  \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,760,746\n",
            "Trainable params: 25,760,746\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 27s 10ms/step - loss: 1.3327 - accuracy: 0.7225 - val_loss: 0.1357 - val_accuracy: 0.9581\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0977 - accuracy: 0.9700 - val_loss: 0.0825 - val_accuracy: 0.9730\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0514 - accuracy: 0.9837 - val_loss: 0.0780 - val_accuracy: 0.9753\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0295 - accuracy: 0.9904 - val_loss: 0.0744 - val_accuracy: 0.9784\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.0726 - val_accuracy: 0.9810\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.1153 - val_accuracy: 0.9721\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.0767 - val_accuracy: 0.9814\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0729 - val_accuracy: 0.9839\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0858 - val_accuracy: 0.9816\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0894 - val_accuracy: 0.9820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "id": "TCp89GeB_3C4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fcd033b-6144-43cc-83a1-e1d03d711ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.08935221284627914\n",
            "Test accuracy: 0.9819999933242798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment:** Tune the CNN\n",
        "2.1: Add convolutional layers, change kernel and filter sizes to get to a 98% accuracy on the test dataset\n",
        "\n",
        "2.2: Go back to the data loader and change the MNIST loader to the Fashion MNIST loader \"fashion_mnist.load_data()\". Tune the parameters until you reach an accuracy of 87% on the test dataset.\n",
        "\n",
        "2.3 (Advanced): Include max-pooling, dropout, and batch normalization layers to reach a test accuracy of 90% on the test set.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKVdjm3AAM2d"
      }
    }
  ]
}